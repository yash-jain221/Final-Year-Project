{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -r require.txt\n",
    "# !python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !C:\\Users\\JainYashVija\\Desktop\\Projects\\Learnings\\data-drift-nlp\\data-drift-nlp-env\\Scripts\\pip.exe install -r require.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JainYashVija\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JainYashVija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JainYashVija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, LSTM,  Embedding\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import scale\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "glove_wiki = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reviews = pd.read_csv(\"../data/Reviews.csv\")\n",
    "data_reviews.drop_duplicates(keep=\"first\",inplace=True)\n",
    "data_reviews = data_reviews.iloc[:,[6,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JainYashVija\\AppData\\Local\\Temp\\ipykernel_13732\\2939592335.py:1: DtypeWarning: Columns (1,2,3,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_twitter_biden = pd.read_csv(\"../data/hashtag_joebiden.csv\")\n",
      "C:\\Users\\JainYashVija\\AppData\\Local\\Temp\\ipykernel_13732\\2939592335.py:2: DtypeWarning: Columns (1,3,6,11,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_twitter_trump = pd.read_csv(\"../data/hashtag_donaldtrump.csv\")\n"
     ]
    }
   ],
   "source": [
    "data_twitter_biden = pd.read_csv(\"../data/hashtag_joebiden.csv\")\n",
    "data_twitter_trump = pd.read_csv(\"../data/hashtag_donaldtrump.csv\")\n",
    "data_twitter = pd.concat([data_twitter_trump[\"tweet\"],data_twitter_biden[\"tweet\"]], axis=0, ignore_index=True)\n",
    "data_twitter = data_twitter.dropna()\n",
    "data_twitter = data_twitter.sample(frac=1)\n",
    "data_twitter = data_twitter.reset_index()\n",
    "data_twitter.drop('index',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@realDonaldTrump \\n@JoeBiden \\n#ThePostElectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What arrogance..to think your view is the only...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@JimmyPatronis @60Minutes @realDonaldTrump @CB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y acab√≥ el terror #Trump esperen los pedos que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>France celebrates the fall of the house of #Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Values, decency, respect are disappearing. Ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@Sethrogen l made these pics of #DonaldTrump a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Biden leads in Michigan! Can he flip this key...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@KatyTurNBC #JoeBiden #ElectionNight https://t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>In Nevada √® rimasta una contea democratica da ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>#DonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>üòÇüëèüèª #trump #biden #Election2020 https://t.co/N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>üì∫üá∫üá∏üó≥ - Novo video na #webTV. Clique no link pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet\n",
       "0   @realDonaldTrump \\n@JoeBiden \\n#ThePostElectio...\n",
       "1   What arrogance..to think your view is the only...\n",
       "2   @JimmyPatronis @60Minutes @realDonaldTrump @CB...\n",
       "3   Y acab√≥ el terror #Trump esperen los pedos que...\n",
       "4   France celebrates the fall of the house of #Tr...\n",
       "5                                                 0.0\n",
       "6                                                 0.0\n",
       "7   Values, decency, respect are disappearing. Ame...\n",
       "8   @Sethrogen l made these pics of #DonaldTrump a...\n",
       "9   #Biden leads in Michigan! Can he flip this key...\n",
       "10  @KatyTurNBC #JoeBiden #ElectionNight https://t...\n",
       "11  In Nevada √® rimasta una contea democratica da ...\n",
       "12                                       #DonaldTrump\n",
       "13  üòÇüëèüèª #trump #biden #Election2020 https://t.co/N...\n",
       "14  üì∫üá∫üá∏üó≥ - Novo video na #webTV. Clique no link pa..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_twitter.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>I don't know if it's the cactus or the tequila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>One of my boys needed to lose some weight and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>My cats have been happily eating Felidae Plati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>good flavor! these came securely packed... the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>The Strawberry Twizzlers are my guilty pleasur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Score                                               Text\n",
       "0       5  I have bought several of the Vitality canned d...\n",
       "1       1  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2       4  This is a confection that has been around a fe...\n",
       "3       2  If you are looking for the secret ingredient i...\n",
       "4       5  Great taffy at a great price.  There was a wid...\n",
       "5       4  I got a wild hair for taffy and ordered this f...\n",
       "6       5  This saltwater taffy had great flavors and was...\n",
       "7       5  This taffy is so good.  It is very soft and ch...\n",
       "8       5  Right now I'm mostly just sprouting this so my...\n",
       "9       5  This is a very healthy dog food. Good for thei...\n",
       "10      5  I don't know if it's the cactus or the tequila...\n",
       "11      5  One of my boys needed to lose some weight and ...\n",
       "12      1  My cats have been happily eating Felidae Plati...\n",
       "13      4  good flavor! these came securely packed... the...\n",
       "14      5  The Strawberry Twizzlers are my guilty pleasur..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reviews.head(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence tokenization and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(data,r: list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    corpus = []\n",
    "    \n",
    "    for i in range(r[0], r[1]):\n",
    "        review = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",data['Text'][i]).lower().split()\n",
    "        review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "        review = ' '.join(review)\n",
    "        corpus.append(review)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning_twitter(data, r: list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    corpus = []\n",
    "    \n",
    "    for i in range(r[0], r[1]):\n",
    "        review = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",data_twitter.iloc[i,0]).lower().split()\n",
    "        review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "        review = ' '.join(review)\n",
    "        corpus.append(review)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(filename, data):\n",
    "    filename = filename\n",
    "    data = data\n",
    "\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        outfile.write('\\n'.join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    with open(filename) as f:\n",
    "        corpus = [line for line in f.readlines()]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_train = text_cleaning(data_reviews,[0,10000])\n",
    "# corpus_test = text_cleaning(data_reviews, [60000,70000])\n",
    "\n",
    "# save_file(\"corpus_test.txt\",corpus_test)\n",
    "# save_file(\"corpus_train.txt\",corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_twitter_train = text_cleaning_twitter(data_twitter, [0,10000])\n",
    "# corpus_twitter_test = text_cleaning_twitter(data_twitter, [60000,70000])\n",
    "\n",
    "# save_file(\"corpus_twitter_train.txt\",corpus_twitter_train)\n",
    "# save_file(\"corpus_twitter_test.txt\",corpus_twitter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = load_file(\"corpus_train.txt\")\n",
    "corpus_test = load_file(\"corpus_test.txt\")\n",
    "corpus_drifted_test = load_file(\"corpus_test_drift_ingested.txt\")\n",
    "\n",
    "corpus_twitter_train = load_file(\"corpus_twitter_train.txt\")\n",
    "corpus_twitter_test = load_file(\"corpus_twitter_test.txt\")\n",
    "corpus_twitter_drifted_test = load_file(\"corpus_twitter_test_drift_ingested.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', min_df=15) \n",
    "    vector = vectorizer.fit_transform(corpus)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    sums = vector.sum(axis=0)\n",
    "    data = []\n",
    "    for col, term in enumerate(terms):\n",
    "        data.append( (term, sums[0,col] ))\n",
    "\n",
    "    ranking = pd.DataFrame(data, columns=['term','rank'])\n",
    "    ranking.sort_values('rank', ascending=False, inplace=True)\n",
    "    ranking.reset_index(inplace=True)\n",
    "    ranking.drop('index', axis=1, inplace=True)\n",
    "    print(ranking.head(20))\n",
    "    return list(ranking.iloc[:20,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drift_ingestion(corpus_train):\n",
    "    corpus = []\n",
    "    with open('corpus_twitter_test.txt') as f:\n",
    "        words_to_replace = get_top_n_words(corpus_train)\n",
    "        for line in f.readlines():\n",
    "            big_regex = re.compile('|'.join(map(re.escape, words_to_replace)))\n",
    "            line = big_regex.sub(\"<replaced>\", line)\n",
    "            corpus.append(line)\n",
    "            \n",
    "    save_file(\"corpus_twitter_test_drift_ingested.txt\",corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               term        rank\n",
      "0             trump  978.177660\n",
      "1             biden  730.413562\n",
      "2          joebiden  459.071539\n",
      "3       donaldtrump  266.965351\n",
      "4      election2020  250.576550\n",
      "5              vote  238.512410\n",
      "6                la  175.999538\n",
      "7         president  167.720342\n",
      "8          election  166.916309\n",
      "9     elections2020  164.079781\n",
      "10              amp  145.634621\n",
      "11  bidenharris2020  142.237255\n",
      "12        trump2020  141.251326\n",
      "13              usa  139.284083\n",
      "14               le  122.289898\n",
      "15          america  117.248508\n",
      "16               en  116.794041\n",
      "17              win  116.789772\n",
      "18      electionday  114.624699\n",
      "19              joe  110.371409\n"
     ]
    }
   ],
   "source": [
    "drift_ingestion(corpus_twitter_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Bert Transformer (SBert) Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding_train,embedding_test):\n",
    "    cosine_scores = util.cos_sim(embedding_train, embedding_test)\n",
    "    return np.mean(np.mean(np.absolute(np.array(cosine_scores)),axis=1),axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedding_train = bert_model.encode(corpus_train, convert_to_tensor=True)\n",
    "embedding_test = bert_model.encode(corpus_test, convert_to_tensor=True)\n",
    "embedding_drifted_test = bert_model.encode(corpus_drifted_test, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2820735"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(embedding_train,embedding_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15698944"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(embedding_train,embedding_drifted_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedding_twitter_train = bert_model.encode(corpus_twitter_train, convert_to_tensor=True)\n",
    "embedding_twitter_test = bert_model.encode(corpus_twitter_test, convert_to_tensor=True)\n",
    "embedding_twitter_drifted_test = bert_model.encode(corpus_twitter_drifted_test, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25215048"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(embedding_twitter_train,embedding_twitter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1552168"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(embedding_twitter_train,embedding_twitter_drifted_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Validation Using LSTM and Keras Embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(corpus):\n",
    "    vocabulary_size = 10000\n",
    "    tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    sequences = tokenizer.texts_to_sequences(corpus)\n",
    "    data = pad_sequences(sequences, maxlen=200)\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_with_labels(train,test):\n",
    "    train['labels'] = 1\n",
    "    test['labels'] = 0\n",
    "    all_data = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "    all_data_shuffled = all_data.sample(frac=1)\n",
    "\n",
    "    X = all_data_shuffled.drop(['labels'], axis=1)\n",
    "    y = all_data_shuffled['labels']\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_run_model(train,test):\n",
    "    X,y = create_df_with_labels(train,test)\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(10000, 200, input_length=200))\n",
    "    model.add(LSTM(64, dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    print('Training the RNN')\n",
    "    model.fit(X,y,validation_split=0.4, epochs=100,callbacks=[callback])\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the RNN\n",
      "Epoch 1/100\n",
      "375/375 [==============================] - 100s 260ms/step - loss: 0.5879 - accuracy: 0.6768 - val_loss: 0.5441 - val_accuracy: 0.7114\n",
      "Epoch 2/100\n",
      "375/375 [==============================] - 90s 241ms/step - loss: 0.4160 - accuracy: 0.8088 - val_loss: 0.5301 - val_accuracy: 0.7387\n",
      "Epoch 3/100\n",
      "375/375 [==============================] - 94s 250ms/step - loss: 0.2807 - accuracy: 0.8860 - val_loss: 0.6178 - val_accuracy: 0.7319\n",
      "Epoch 4/100\n",
      "375/375 [==============================] - 98s 261ms/step - loss: 0.1710 - accuracy: 0.9362 - val_loss: 0.6941 - val_accuracy: 0.7467\n",
      "Epoch 5/100\n",
      "375/375 [==============================] - 102s 272ms/step - loss: 0.0939 - accuracy: 0.9672 - val_loss: 0.8283 - val_accuracy: 0.7517\n",
      "Epoch 6/100\n",
      "375/375 [==============================] - 92s 246ms/step - loss: 0.0663 - accuracy: 0.9769 - val_loss: 1.0375 - val_accuracy: 0.7418\n",
      "Epoch 7/100\n",
      "375/375 [==============================] - 73s 195ms/step - loss: 0.0354 - accuracy: 0.9891 - val_loss: 1.0648 - val_accuracy: 0.7620\n",
      "Epoch 8/100\n",
      "375/375 [==============================] - 73s 194ms/step - loss: 0.0234 - accuracy: 0.9930 - val_loss: 1.0502 - val_accuracy: 0.7674\n",
      "Epoch 9/100\n",
      "375/375 [==============================] - 73s 194ms/step - loss: 0.0307 - accuracy: 0.9904 - val_loss: 1.1096 - val_accuracy: 0.7610\n",
      "Epoch 10/100\n",
      "375/375 [==============================] - 73s 194ms/step - loss: 0.0131 - accuracy: 0.9968 - val_loss: 1.1881 - val_accuracy: 0.7671\n",
      "Epoch 11/100\n",
      "375/375 [==============================] - 72s 193ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 1.3607 - val_accuracy: 0.7631\n",
      "Epoch 12/100\n",
      "375/375 [==============================] - 70s 187ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 1.3830 - val_accuracy: 0.7631\n",
      "Epoch 13/100\n",
      "375/375 [==============================] - 75s 199ms/step - loss: 0.0206 - accuracy: 0.9942 - val_loss: 1.1312 - val_accuracy: 0.7615\n",
      "Epoch 14/100\n",
      "375/375 [==============================] - 73s 195ms/step - loss: 0.0243 - accuracy: 0.9927 - val_loss: 1.1463 - val_accuracy: 0.7678\n",
      "Epoch 15/100\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 1.2448 - val_accuracy: 0.7731\n",
      "Epoch 16/100\n",
      "375/375 [==============================] - 68s 182ms/step - loss: 9.5764e-04 - accuracy: 1.0000 - val_loss: 1.4004 - val_accuracy: 0.7719\n",
      "Epoch 17/100\n",
      "375/375 [==============================] - 64s 172ms/step - loss: 0.0147 - accuracy: 0.9955 - val_loss: 1.2816 - val_accuracy: 0.7606\n",
      "Epoch 18/100\n",
      "375/375 [==============================] - 72s 192ms/step - loss: 0.0073 - accuracy: 0.9978 - val_loss: 1.2496 - val_accuracy: 0.7741\n",
      "Epoch 19/100\n",
      "375/375 [==============================] - 73s 194ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 1.3417 - val_accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "data_train = text_preprocessing(corpus_train)\n",
    "data_test = text_preprocessing(corpus_test)\n",
    "\n",
    "model = build_and_run_model(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.910099983215332\n",
      "0.9121000170707703\n"
     ]
    }
   ],
   "source": [
    "_, test_acc = model.evaluate(data_test.iloc[:,:-1], data_test.iloc[:,-1], verbose=0)\n",
    "_, train_acc = model.evaluate(data_train.iloc[:,:-1], data_train.iloc[:,-1], verbose=0)\n",
    "\n",
    "print(test_acc)\n",
    "print(train_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the RNN\n",
      "Epoch 1/100\n",
      "375/375 [==============================] - 101s 262ms/step - loss: 0.6058 - accuracy: 0.6477 - val_loss: 0.5307 - val_accuracy: 0.7214\n",
      "Epoch 2/100\n",
      "375/375 [==============================] - 306s 817ms/step - loss: 0.3827 - accuracy: 0.8163 - val_loss: 0.4946 - val_accuracy: 0.7554\n",
      "Epoch 3/100\n",
      "375/375 [==============================] - 298s 795ms/step - loss: 0.2002 - accuracy: 0.9073 - val_loss: 0.6075 - val_accuracy: 0.7582\n",
      "Epoch 4/100\n",
      "375/375 [==============================] - 209s 557ms/step - loss: 0.1253 - accuracy: 0.9367 - val_loss: 0.7292 - val_accuracy: 0.7551\n",
      "Epoch 5/100\n",
      "375/375 [==============================] - 198s 528ms/step - loss: 0.0987 - accuracy: 0.9499 - val_loss: 0.8521 - val_accuracy: 0.7467\n",
      "Epoch 6/100\n",
      "375/375 [==============================] - 314s 838ms/step - loss: 0.0842 - accuracy: 0.9537 - val_loss: 0.9410 - val_accuracy: 0.7599\n",
      "Epoch 7/100\n",
      "375/375 [==============================] - 324s 864ms/step - loss: 0.0750 - accuracy: 0.9549 - val_loss: 0.9293 - val_accuracy: 0.7610\n",
      "Epoch 8/100\n",
      "375/375 [==============================] - 331s 884ms/step - loss: 0.0689 - accuracy: 0.9585 - val_loss: 1.0217 - val_accuracy: 0.7535\n",
      "Epoch 9/100\n",
      "375/375 [==============================] - 326s 869ms/step - loss: 0.0697 - accuracy: 0.9602 - val_loss: 1.0393 - val_accuracy: 0.7585\n",
      "Epoch 10/100\n",
      "375/375 [==============================] - 323s 860ms/step - loss: 0.0638 - accuracy: 0.9602 - val_loss: 1.0342 - val_accuracy: 0.7659\n",
      "Epoch 11/100\n",
      "375/375 [==============================] - 261s 697ms/step - loss: 0.0620 - accuracy: 0.9603 - val_loss: 1.1630 - val_accuracy: 0.7523\n",
      "Epoch 12/100\n",
      "375/375 [==============================] - 411s 1s/step - loss: 0.0600 - accuracy: 0.9616 - val_loss: 1.1808 - val_accuracy: 0.7555\n",
      "Epoch 13/100\n",
      "375/375 [==============================] - 369s 983ms/step - loss: 0.0579 - accuracy: 0.9637 - val_loss: 1.3140 - val_accuracy: 0.7585\n",
      "Epoch 14/100\n",
      "375/375 [==============================] - 363s 968ms/step - loss: 0.0563 - accuracy: 0.9628 - val_loss: 1.3467 - val_accuracy: 0.7571\n",
      "Epoch 15/100\n",
      "375/375 [==============================] - 362s 966ms/step - loss: 0.0651 - accuracy: 0.9600 - val_loss: 1.1076 - val_accuracy: 0.7602\n",
      "Epoch 16/100\n",
      "375/375 [==============================] - 375s 1000ms/step - loss: 0.0584 - accuracy: 0.9613 - val_loss: 1.1949 - val_accuracy: 0.7517\n",
      "Epoch 17/100\n",
      "375/375 [==============================] - 359s 958ms/step - loss: 0.0572 - accuracy: 0.9614 - val_loss: 1.2721 - val_accuracy: 0.7563\n"
     ]
    }
   ],
   "source": [
    "data_twitter_train = text_preprocessing(corpus_twitter_train)\n",
    "data_twitter_test = text_preprocessing(corpus_twitter_test)\n",
    "\n",
    "model = build_and_run_model(data_twitter_train,data_twitter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8677999973297119\n",
      "0.8962000012397766\n"
     ]
    }
   ],
   "source": [
    "_, test_acc_twitter = model.evaluate(data_twitter_test.iloc[:,:-1], data_twitter_test.iloc[:,-1], verbose=0)\n",
    "_, train_acc_twitter = model.evaluate(data_twitter_train.iloc[:,:-1], data_twitter_train.iloc[:,-1], verbose=0)\n",
    "\n",
    "print(test_acc_twitter)\n",
    "print(train_acc_twitter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GloVe(text, size, vectors, aggregation='mean'):\n",
    "    vec = np.zeros(size).reshape((1, size)) \n",
    "    count = 0\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            vec += vectors[word].reshape((1, size)) \n",
    "            count += 1 \n",
    "        except KeyError:\n",
    "            continue\n",
    "    if aggregation == 'mean':\n",
    "        if count != 0:\n",
    "            vec /= count  #get average of vector to create embedding for sentence\n",
    "        return vec\n",
    "    elif aggregation == 'sum':\n",
    "        return vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_embeddings_train = scale(np.concatenate([get_GloVe(text,50,glove_wiki) for text in corpus_train]))\n",
    "# glove_embeddings_test = scale(np.concatenate([get_GloVe(text,50,glove_wiki) for text in corpus_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"glove_embeddings_train.txt\",glove_embeddings_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings_train = np.loadtxt('glove_embeddings_train.txt')\n",
    "glove_embeddings_test = np.loadtxt('glove_embeddings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18863798703748552"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove_embeddings_train,glove_embeddings_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_embeddings_twitter_train = scale(np.concatenate([get_GloVe(text,50,glove_wiki) for text in corpus_twitter_train]))\n",
    "# glove_embeddings_twitter_test = scale(np.concatenate([get_GloVe(text,50,glove_wiki) for text in corpus_twitter_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"glove_embeddings_twitter_test.txt\",glove_embeddings_twitter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings_twitter_train = np.loadtxt('glove_embeddings_twitter_train.txt')\n",
    "glove_embeddings_twitter_test = np.loadtxt('glove_embeddings_twitter_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2288196098694682"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(glove_embeddings_twitter_train,glove_embeddings_twitter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-drift-nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2502099d4b52e4bee41449a8ad3cd00c6331ba2e1198d3ad56070899a9c93f84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
